{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datathon Passos Mágicos - Análise de Risco Educacional\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Este notebook consolida todo o processo de análise de dados e desenvolvimento do modelo preditivo de risco educacional para a Associação Passos Mágicos, conforme os requisitos do Datathon - Fase 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FASE 1: Preparação e Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(file_path):\n",
    "    sheet_to_year = {\n",
    "        'PEDE2022': 2022,\n",
    "        'PEDE2023': 2023,\n",
    "        'PEDE2024': 2024\n",
    "    }\n",
    "    all_data = []\n",
    "    for sheet_name, year in sheet_to_year.items():\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            df['Ano'] = year\n",
    "            df.columns = df.columns.str.lower().str.replace('[^a-zA-Z0-9_]', '', regex=True)\n",
    "            if f'pedra{year}' in df.columns:\n",
    "                df = df.rename(columns={f'pedra{year}': 'fase_pedra_ano'})\n",
    "            all_data.append(df)\n",
    "        except ValueError as e:\n",
    "            print(f\"Aba '{sheet_name}' não encontrada ou erro de leitura: {e}\")\n",
    "    if not all_data:\n",
    "        return None\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    for col in combined_df.select_dtypes(include=['object']).columns:\n",
    "        combined_df[col] = combined_df[col].str.strip()\n",
    "    combined_df.dropna(how='all', inplace=True)\n",
    "    if 'ra' in combined_df.columns:\n",
    "        combined_df['aluno_id'] = combined_df['ra']\n",
    "    return combined_df\n",
    "\n",
    "base_dir = '..\'\n",
    "raw_data_path = os.path.join(base_dir, 'data', 'raw', 'BASE DE DADOS PEDE 2024 - DATATHON.xlsx')\n",
    "processed_data_path = os.path.join(base_dir, 'data', 'processed', 'pedagogy_data_clean.csv')\n",
    "os.makedirs(os.path.join(base_dir, 'data', 'processed'), exist_ok=True)\n",
    "clean_df = load_and_clean_data(raw_data_path)\n",
    "if clean_df is not None:\n",
    "    clean_df.to_csv(processed_data_path, index=False)\n",
    "    print(f'Dados limpos salvos em: {processed_data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FASE 2: Análise Exploratória de Dados (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(processed_data_path)\n",
    "df['gnero'] = df['gnero'].replace({'Menina': 'Feminino', 'Menino': 'Masculino'})\n",
    "core_cols = [\n",
    "    'aluno_id', 'ano', 'gnero', 'fase', 'anoingresso',\n",
    "    'ida', 'ieg', 'ips', 'ipp', 'iaa', 'ipv', 'ian', 'inde22', 'inde23',\n",
    "    'mat', 'por', 'ing', 'matem', 'portug', 'ingls',\n",
    "    'defasagem', 'defas',\n",
    "    'instituiodeensino', 'escola',\n",
    "    'ativoinativo', 'ativoinativo1'\n",
    "]\n",
    "df_clean = df[df.columns.intersection(core_cols)].copy()\n",
    "df_clean['risco_defasagem'] = np.where(df_clean['ian'] < 7.0, 1, 0)\n",
    "df_clean['fase'] = df_clean['fase'].fillna('NAO_INFORMADO')\n",
    "df_clean['gnero'] = df_clean['gnero'].fillna('NAO_INFORMADO')\n",
    "final_data_path = os.path.join(base_dir, 'data', 'processed', 'pedagogy_data_final.csv')\n",
    "df_clean.to_csv(final_data_path, index=False)\n",
    "print(f'Dados finais salvos em: {final_data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FASE 3: Feature Engineering e Preparação para Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(final_data_path)\n",
    "df_fe = df.copy()\n",
    "df_fe['ida_ieg_interaction'] = df_fe['ida'] * df_fe['ieg']\n",
    "df_fe['ipp_ips_interaction'] = df_fe['ipp'] * df_fe['ips']\n",
    "df_fe['ida_ieg_ratio'] = df_fe['ida'] / (df_fe['ieg'] + 0.001)\n",
    "df_fe['mean_indicators'] = df_fe[['ida', 'ieg', 'ips', 'ipp', 'iaa', 'ipv']].mean(axis=1, skipna=True)\n",
    "df_fe = df_fe.sort_values(['aluno_id', 'ano'])\n",
    "df_fe['ida_lag1'] = df_fe.groupby('aluno_id')['ida'].shift(1)\n",
    "df_fe['ieg_lag1'] = df_fe.groupby('aluno_id')['ieg'].shift(1)\n",
    "df_fe['ips_lag1'] = df_fe.groupby('aluno_id')['ips'].shift(1)\n",
    "df_fe['ida_delta'] = df_fe['ida'] - df_fe['ida_lag1']\n",
    "df_fe['ieg_delta'] = df_fe['ieg'] - df_fe['ieg_lag1']\n",
    "le_genero = LabelEncoder()\n",
    "df_fe['gnero_encoded'] = le_genero.fit_transform(df_fe['gnero'].fillna('NAO_INFORMADO'))\n",
    "le_fase = LabelEncoder()\n",
    "df_fe['fase_encoded'] = le_fase.fit_transform(df_fe['fase'].fillna('NAO_INFORMADO'))\n",
    "fe_data_path = os.path.join(base_dir, 'data', 'processed', 'pedagogy_data_fe.csv')\n",
    "df_fe.to_csv(fe_data_path, index=False)\n",
    "print(f'DataFrame com feature engineering salvo em: {fe_data_path}')"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FASE 4: Treinamento e Avaliação de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.read_csv(fe_data_path)\n",
    "feature_cols = [\n",
    "    'ida', 'ieg', 'ips', 'ipp', 'iaa', 'ipv',\n",
    "    'ida_ieg_interaction', 'ipp_ips_interaction', 'ida_ieg_ratio',\n",
    "    'mean_indicators', 'ida_lag1', 'ieg_lag1', 'ips_lag1',\n",
    "    'ida_delta', 'ieg_delta', 'gnero_encoded', 'fase_encoded', 'ano'\n",
    "]\n",
    "target_col = 'risco_defasagem'\n",
    "df_model = df_model[df_model[target_col].notna()].copy()\n",
    "df_model = df_model.dropna(subset=['ida', 'ieg', 'ipp', 'iaa', 'ipv'], thresh=4)\n",
    "for col in feature_cols:\n",
    "    if col in df_model.columns:\n",
    "        df_model[col] = df_model[col].fillna(df_model[col].median())\n",
    "X = df_model[feature_cols]\n",
    "y = df_model[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
    "}\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_pred_proba)\n",
    "    })\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FASE 5: Otimização e Interpretação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "joblib.dump(best_model, os.path.join(base_dir, 'src', 'optimized_model.pkl'))\n",
    "joblib.dump(scaler, os.path.join(base_dir, 'src', 'scaler_v2.pkl'))\n",
    "joblib.dump(feature_cols, os.path.join(base_dir, 'src', 'feature_cols.pkl'))\n",
    "\n",
    "feature_importance = best_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_cols, 'Importance': feature_importance}).sort_values('Importance', ascending=False)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=feature_importance_df.head(15), x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('Top 15 Features Mais Importantes - Gradient Boosting')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
